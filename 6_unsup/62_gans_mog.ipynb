{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1b7e494df820d1e1232d7aa45d05840",
     "grade": false,
     "grade_id": "cell-1874cc59e27a5440",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise 6. Part 2. Generative adversarial networks\n",
    "\n",
    "## Learning goals\n",
    "* get familiar with measures of difference between two probability distributions\n",
    "* get familiar with Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65e2970339980ef7d85c3754662c4ee8",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acefd9d6e9f3a0de4f29a1a1b5639f58",
     "grade": false,
     "grade_id": "cell-96825b3f4d457d23",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll run this exercise on CPU\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3564a3b883c277bc3476d5248e0ac0d0",
     "grade": false,
     "grade_id": "cell-bdde3f5e53b9203d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Synthetic data\n",
    "\n",
    "Our training data will be sampled from a distribution that is a mixture of two Gaussian distributions:\n",
    "$$\n",
    "  p(x) = w_1 N(x \\mid \\mu_1, \\sigma_1) + w_2 N(x \\mid \\mu_2, \\sigma_2) .\n",
    "$$\n",
    "This will be the target distribution that we want to model with Generative Adversarial Networks.\n",
    "\n",
    "The cell below defines the target probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a9af2db9b901d90cb4e7658c3f38a0e",
     "grade": false,
     "grade_id": "cell-5d0cdb6e023f5e23",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class MoG:\n",
    "    def __init__(self, m=np.array([-0.5, 0.5]), w=[0.5, 0.5]):\n",
    "        \"Probability distribution which is a mixture of two Gaussians.\"\n",
    "        self.w = w\n",
    "        self.m = m\n",
    "        self.sigma = np.array([0.2, 0.2])\n",
    "\n",
    "    def logpdf(self, x):\n",
    "        \"Log-probability density function\"\n",
    "        w, m, sigma = self.w, self.m, self.sigma\n",
    "        return np.logaddexp(\n",
    "            np.log(w[0]) + norm.logpdf(x, loc=m[0], scale=sigma[0]),\n",
    "            np.log(w[1]) + norm.logpdf(x, loc=m[1], scale=sigma[1])\n",
    "        )\n",
    "\n",
    "    def rvs(self, size):\n",
    "        \"Generate samples from the distribution.\"\n",
    "        x = norm.rvs(0, 1, size)\n",
    "        z = (np.random.rand(size) <= 1-self.w[0]).astype(int)\n",
    "        return x * self.sigma[z] + self.m[z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd4787a86e77ddffc5f87ce4700b4f2e",
     "grade": false,
     "grade_id": "cell-da7119fc14565724",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's generate training data from the target distribution\n",
    "p_data = MoG()\n",
    "n_samples = 100\n",
    "data = p_data.rvs(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c2032e9bfd553e5face8b18fadbe51b",
     "grade": false,
     "grade_id": "cell-ecf280b0bad09b7e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's visualize generated samples\n",
    "fig, ax = plt.subplots(1)\n",
    "x = np.linspace(-1, 1, 100)\n",
    "ax.plot(x, np.exp(p_data.logpdf(x)))\n",
    "ax.plot(data, np.zeros_like(data), '.')\n",
    "ax.set_ylim(-0.1, 1.3)\n",
    "ax.legend(['pdf of target distribution', 'training data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b4d37502c85a45a6100919cc71a63be",
     "grade": false,
     "grade_id": "cell-c5756935dd3d3e10",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Generative adversarial networks\n",
    "\n",
    "Our task is to train a generative model of the data, that is a model from which we can draw samples that will have a distribution similar to the distribution of the training data (shown in orange on the plot above).\n",
    "\n",
    "### Generator\n",
    "The generative model that we are going to train is:\n",
    "\\begin{align}\n",
    "z &\\sim N(0, I)\n",
    "\\\\\n",
    "x &= g(z)\n",
    "\\end{align}\n",
    "that is the data is generated by applying a nonlinear transformation to samples drawn from the standard normal distribution.\n",
    "\n",
    "We are going to model $g$ with a deep neural network created below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ea6309fbb77ab31b357c709030ae193",
     "grade": false,
     "grade_id": "cell-77af4190c211c399",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size=2):\n",
    "        super(Generator, self).__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(latent_size, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7b0b5ea2e82492bd887f81ba3281133",
     "grade": false,
     "grade_id": "cell-1e3e00832c1b3d77",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Discriminator\n",
    "\n",
    "The generative model will be guided by a discriminator whose task is to separate (classify) data into two classes:\n",
    "* true data (samples from the training set)\n",
    "* generated data (samples generated by the generator).\n",
    "\n",
    "The task of the generator is to confuse the discriminator as much as possible, which is the case when the distribution produced by the generator perfectly replicates the data distribution.\n",
    "\n",
    "We are going to use a deep neural network as the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e15f999c10bf10f754ed181a0eeab3a8",
     "grade": false,
     "grade_id": "cell-967311c7a3e31459",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(1, 50),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(50, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "27cf084047a6140ebcb0e15e7f38531d",
     "grade": false,
     "grade_id": "cell-98a1b6534db1c26d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Training GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5925bda4f7ecf8ce9130a8bccaae378",
     "grade": false,
     "grade_id": "cell-288c6d57cda8365f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "latent_size = 2\n",
    "G = Generator(latent_size)\n",
    "D = Discriminator()\n",
    "\n",
    "D = D.to(device)\n",
    "G = G.to(device)\n",
    "\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a3b52620a5228b6f853b5958c8932a5",
     "grade": false,
     "grade_id": "cell-8dfe35ec99565aa8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "One iteration of our algorithm to train GANs is:\n",
    "* Train the discriminator:\n",
    "    * Generate fake data using the generator (generate the same number of samples as the\n",
    "    number of real examples).\n",
    "    * Use `criterion = nn.BCELoss()` defined below to compute the cost function.\n",
    "    * Use all ones (`targets_real` defined below) as targets for the real data.\n",
    "    * Use all zeroes (`targets_fake` defined below) as targets for the fake data.\n",
    "    * Update the parameters of the discriminator using gradient descent.\n",
    "* Train the generator:\n",
    "    * Generate fake data using the generator\n",
    "    * Update the parameters of the generator using gradient descent.\n",
    "\n",
    "The update of the generator has already been implemented. Your task is to insert the part that updates the discriminator. You may also want to update the discriminator a few times for better convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39b1347eebb5b98d5b0de4a159a0e649",
     "grade": false,
     "grade_id": "cell-954e9475b37e4c5f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Binary cross entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "data_real = torch.Tensor(data).view(n_samples, 1).to(device)\n",
    "targets_real = torch.ones(n_samples, 1).to(device)  # Targets for discriminator: real data\n",
    "targets_fake = torch.zeros(n_samples, 1).to(device)  # Targets for discriminator: fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000  # You can increase the number of epochs if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0bfebba5ebe5764550c3b502279155d",
     "grade": false,
     "grade_id": "cell-db1b8467a47deaff",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)  # Progress plot\n",
    "scheduler = StepLR(g_optimizer, step_size=100, gamma=0.95)\n",
    "for epoch in range(n_epochs):\n",
    "    scheduler.step()\n",
    "    # Train the discriminator\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Train the generator\n",
    "    # Compute loss with fake data\n",
    "    z = torch.randn(n_samples, latent_size).to(device)\n",
    "    data_fake = G(z)\n",
    "    outputs = D(data_fake)\n",
    "    g_loss = criterion(outputs, targets_real)\n",
    "\n",
    "    g_optimizer.zero_grad()\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "\n",
    "    if skip_training:\n",
    "        break\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        # Plot progress\n",
    "        ax.clear()\n",
    "\n",
    "        # Plot probability density function of the target distribution\n",
    "        x = np.linspace(-1, 1, 100)\n",
    "        h1, = ax.plot(x, np.exp(p_data.logpdf(x)))\n",
    "\n",
    "        # Plot estimate of the model pdf\n",
    "        z = torch.randn(1000, latent_size).to(device)\n",
    "        data_g = G(z)\n",
    "        data_g = data_g.detach().cpu().numpy()\n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth=0.08).fit(data_g)\n",
    "        log_pdf = kde.score_samples(x.reshape((-1, 1)))\n",
    "        h2, = ax.plot(x, np.exp(log_pdf), 'g-')\n",
    "\n",
    "        ax.plot(data_fake.detach().cpu().numpy(), np.zeros(n_samples), 'g.')\n",
    "\n",
    "        d_output = D(torch.Tensor(x).view(-1, 1).to(device))\n",
    "        h3, = ax.plot(x, d_output.detach().cpu().numpy(), 'r')\n",
    "\n",
    "        ax.set_ylim([-0.05, 2.5])\n",
    "        ax.set_xlim([-1, 1])\n",
    "        ax.legend((h1, h2, h3), ('data distribution', 'generator', 'discriminator'),\n",
    "                   loc='upper right')\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(fig)\n",
    "        \n",
    "        print('Epoch {}, g_loss: {:.4f}'.format(epoch, g_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53fe2d4b18b0ef9020e16f8ee50d2967",
     "grade": false,
     "grade_id": "cell-1474445115e099f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The generated data distribution should get close to the true data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab274af9bdeca1ed5b7890ac2f2212bb",
     "grade": true,
     "grade_id": "accuracy",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Save the model to disk, submit the files together with your notebook\n",
    "g_filename = '6_gan_generator.pth'\n",
    "d_filename = '6_gan_discriminator.pth'\n",
    "if not skip_training:\n",
    "    try:\n",
    "        do_save = input('Do you want to save the model (type yes to confirm)? ').lower()\n",
    "        if do_save == 'yes':\n",
    "            torch.save(G.state_dict(), g_filename)\n",
    "            torch.save(D.state_dict(), d_filename)\n",
    "            print('Model saved to %s, %s.' % (g_filename, d_filename))\n",
    "        else:\n",
    "            print('Model not saved.')\n",
    "    except:\n",
    "        raise Exception('The notebook should be run or validated with skip_training=True.')\n",
    "else:\n",
    "    G = Generator(latent_size)\n",
    "    G.load_state_dict(torch.load(g_filename, map_location=lambda storage, loc: storage))\n",
    "    print('Generator loaded from %s.' % g_filename)\n",
    "    G = G.to(device)\n",
    "    G.eval()\n",
    "\n",
    "    D = Discriminator()\n",
    "    D.load_state_dict(torch.load(d_filename, map_location=lambda storage, loc: storage))\n",
    "    print('Discriminator loaded from %s.' % d_filename)\n",
    "    D = D.to(device)\n",
    "    D.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5056ac8a2f23364a82b3c2ddcb41d51d",
     "grade": false,
     "grade_id": "cell-c52dc509252f50cb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Measures of difference between two probability distributions\n",
    "\n",
    "According to the [original GAN paper](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf), the GAN training procedure minimizes the Jensen–Shannon divergence (JSD) between the data distribution and the model distribution:\n",
    "$$\n",
    "\\text{JSD}(p_\\text{data} \\:\\lVert\\: p_g) = \\frac{1}{2} \\left[\\text{KL}(p_\\text{data} \\:\\lVert\\: p_*) + \\text{KL}(p_g \\:\\lVert\\: p_*) \\right]\n",
    "$$\n",
    "where $p_*(x) = \\frac{1}{2} \\left[p_\\text{data}(x) + p_g(x)\\right]$\n",
    "and $\\text{KL}(p \\:\\lVert\\: q)$ is the Kullback-Leibler divergence defined as\n",
    "$$\n",
    "  \\text{KL}(p \\:\\lVert\\: q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8371dfacd0466368970764f63e2577d7",
     "grade": false,
     "grade_id": "cell-f7f352e44cceef38",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We are going to track the JSD during training. In order to do that, we need to estimate the KL-divergence between two distributions, which can be done via sampling:\n",
    "$$\n",
    "KL(p \\:\\lVert\\: q) \\approx \\frac{1}{N} \\sum_{i=1}^N \\left[ -\\log q(x_i) + \\log p(x_i) \\right]\n",
    "\\qquad \\text{with} \\quad x_i \\sim p(x)\n",
    "$$\n",
    "\n",
    "In the cell below, your task is to implement a function that estimates the Kullback-Leibler divergence for two given distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e4718faf04a2223056294eb91429a12",
     "grade": false,
     "grade_id": "cell-ce792e00719639b5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def KL(p, q, n=10000):\n",
    "    \"\"\"Estimate of the Kullback-Leibler divergence between distributions p and q.\n",
    "       KL(p || q) = - \\int p(x) \\log q(x) dx + \\int p(x) \\log p(x) dx\n",
    "    \n",
    "    Args:\n",
    "      p and q are objects with the following methods:\n",
    "        p.logpdf(x): Computes log-probability density function for numpy arrays x\n",
    "        p.rvs(n): Draws n random samples from the distribution (returns a numpy array)\n",
    "        See class MoG for an example distribution.\n",
    "      n (int): Number of samples to be drawn from p.\n",
    "    \n",
    "    Returns:\n",
    "      Estimate of the Kullback-Leibler divergence between distributions p and q.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e834620e31da6c8523550a98f50257a",
     "grade": true,
     "grade_id": "kl",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Gaussian:\n",
    "    def __init__(self, m, sigma):\n",
    "        self.m, self.sigma = m, sigma\n",
    "\n",
    "    def logpdf(self, x):\n",
    "        return norm.logpdf(x, self.m, self.sigma)\n",
    "\n",
    "    def entropy(self):\n",
    "        return norm.entropy(self.m, self.sigma)\n",
    "\n",
    "    def rvs(self, size):\n",
    "        return norm.rvs(self.m, self.sigma, size)\n",
    "\n",
    "p1 = Gaussian(0, 1)\n",
    "p2 = Gaussian(0.5, 1)\n",
    "print(KL(p1, p2, n=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc9c0973f10d3d70df93e3e086986b0d",
     "grade": false,
     "grade_id": "cell-3d18c95bea84b209",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Jensen–Shannon divergence\n",
    "class AveragePDFDistribution:\n",
    "    def __init__(self, p1, p2):\n",
    "        \"\"\"Distribution whose pdf is an average the pdfs of two given distributions:\n",
    "            p(x) = 0.5 (p1(x) + p2(x))\n",
    "        \"\"\"\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "\n",
    "    def logpdf(self, x):\n",
    "        return np.logaddexp(self.p1.logpdf(x), self.p2.logpdf(x)) + np.log(0.5)\n",
    "\n",
    "    def rvs(self, n):\n",
    "        return np.vstack((self.p1.rvs(n//2), self.p2.rvs(n//2)))\n",
    "\n",
    "\n",
    "def JSD(p1, p2, n=10000):\n",
    "    \"\"\"Jensen–Shannon divergence\n",
    "        JSD = 0.5 (KL(p1 || 0.5 (p1 + p2)) + KL(p2 || 0.5 (p1 + p2)))\n",
    "    \"\"\"\n",
    "    average_dist = AveragePDFDistribution(p1, p2)\n",
    "    return 0.5 * (KL(p1, average_dist, n) + KL(p2, average_dist, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f34e19dc409a08f7bbada5c7ad1a1df5",
     "grade": false,
     "grade_id": "cell-147e1c86573c3e9d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is the model distribution\n",
    "class GeneratorDistribution:\n",
    "    def __init__(self, G, latent_size):\n",
    "        \"\"\"Distribution modeled by the generator.\"\"\"\n",
    "        self.G = G\n",
    "        self.latent_size = latent_size\n",
    "        self.kde = None\n",
    "\n",
    "    def fit(self):\n",
    "        # Fit kernel density estimator\n",
    "        data_g = self.rvs(1000)\n",
    "        self.kde = KernelDensity(kernel='gaussian', bandwidth=0.08).fit(data_g)\n",
    "\n",
    "    def logpdf(self, x):\n",
    "        log_pdf_x = self.kde.score_samples(x.reshape((-1, 1)))\n",
    "        return log_pdf_x\n",
    "\n",
    "    def rvs(self, n):\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n, self.latent_size).to(device)\n",
    "            data_g = self.G(z)\n",
    "        return data_g.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a07e93b94ba920486524a18effa8d48d",
     "grade": true,
     "grade_id": "cell-f7fa14660a7f1d25",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2da7708d56884f40e47b5bf61312ca8",
     "grade": false,
     "grade_id": "cell-0ec277bdfc3ebfd9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Train GANs again\n",
    "G = Generator(latent_size)\n",
    "D = Discriminator()\n",
    "\n",
    "D = D.to(device)\n",
    "G = G.to(device)\n",
    "# This is the model distribution\n",
    "model_dist = GeneratorDistribution(G, latent_size)\n",
    "\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69e3f88fa2385dcf01dc5896d3d788c1",
     "grade": false,
     "grade_id": "cell-77f02e93870c3cad",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # Progress plot\n",
    "scheduler = StepLR(g_optimizer, step_size=100, gamma=0.95)\n",
    "jsd_history = []\n",
    "for epoch in range(n_epochs):\n",
    "    scheduler.step()\n",
    "    # Train the discriminator\n",
    "    # Copy your code that updates the discriminator here\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Train the generator\n",
    "    # Compute loss with fake data\n",
    "    z = torch.randn(n_samples, latent_size).to(device)\n",
    "    data_fake = G(z)\n",
    "    outputs = D(data_fake)\n",
    "    g_loss = criterion(outputs, targets_real)\n",
    "\n",
    "    g_optimizer.zero_grad()\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "\n",
    "    if skip_training:\n",
    "        break\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        model_dist.fit()\n",
    "        try:\n",
    "            jsd_history.append(JSD(p_data, model_dist, n=500))\n",
    "        except:\n",
    "            jsd_history.append(np.NaN)\n",
    "\n",
    "        # Plot progress\n",
    "        ax = axs[0]\n",
    "        ax.clear()\n",
    "\n",
    "        # Plot probability density function of the target distribution\n",
    "        x = np.linspace(-1, 1, 100)\n",
    "        h1, = ax.plot(x, np.exp(p_data.logpdf(x)))\n",
    "\n",
    "        # Plot estimate of the model pdf\n",
    "        log_pdf = model_dist.logpdf(x)\n",
    "        h2, = ax.plot(x, np.exp(log_pdf), 'g-')\n",
    "\n",
    "        ax.plot(data_fake.detach().cpu().numpy(), np.zeros(n_samples), 'g.')\n",
    "\n",
    "        d_output = D(torch.Tensor(x).view(-1, 1).to(device))\n",
    "        h3, = ax.plot(x, d_output.detach().cpu().numpy(), 'r')\n",
    "\n",
    "        ax.set_ylim([-0.05, 2.5])\n",
    "        ax.set_xlim([-1, 1])\n",
    "        ax.legend((h1, h2, h3), ('data distribution', 'generator', 'discriminator'),\n",
    "                   loc='upper right')\n",
    "\n",
    "        ax = axs[1]\n",
    "        ax.clear()\n",
    "        ax.plot(jsd_history)\n",
    "        ax.grid(True)\n",
    "        ax.set_title('JSD')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(fig)\n",
    "        \n",
    "        print('Epoch {}, g_loss: {:.4f}, jsd: {:.4f}'.format(\n",
    "            epoch, g_loss.item(), jsd_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(jsd_history)\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
